{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive, userdata\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "github_token = userdata.get('github_token')\n",
    "github_username = userdata.get('github_username')\n",
    "try:\n",
    "  wandb_key = userdata.get('wandb_key')\n",
    "except userdata.SecretNotFoundError: \n",
    " wandb_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://$github_username:$github_token@github.com/developer-sidani/CharBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r CharBERT/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!mkdir data\n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
    "\n",
    "# train_text = dataset['train'][:500]['text']\n",
    "# eval_text = dataset['train'][500:650]['text']\n",
    "# test_text = dataset['train'][650:800]['text']\n",
    "\n",
    "\n",
    "# !mkdir data\n",
    "# # Note that if you work on Colab you may need to create the folder \"data\" ---> change the directories based on your development tool of choice\n",
    "# text = ''\n",
    "# for el in train_text:\n",
    "#   text += el\n",
    "# with open(\"/content/data/train.txt\", 'w', encoding='utf-8') as f:\n",
    "#   f.write(text)\n",
    "\n",
    "\n",
    "# text = ''\n",
    "# for el in eval_text:\n",
    "#   text += el\n",
    "# with open(\"/content/data/eval.txt\", 'w', encoding='utf-8') as f:\n",
    "#   f.write(text)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "# text = ''\n",
    "# for el in test_text:\n",
    "#   text += el\n",
    "# with open(\"/content/data/test.txt\", 'w', encoding='utf-8') as f:\n",
    "#   f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /content/CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-cased \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --wandb_key $wandb_key \\\n",
    "    --wandb_project CharBERT \\\n",
    "    --wandb_run_name \"bert_base_cased_wiki_eng\" \\\n",
    "    --train_data_file \"/content/drive/MyDrive/NLP/data/wiki-eng/train.txt\" \\\n",
    "    --eval_data_file  \"/content/drive/MyDrive/NLP/data/wiki-eng/eval.txt\" \\\n",
    "    --term_vocab \"/content/CharBERT/data/dict/term_vocab\" \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --char_vocab \"/content/CharBERT/data/dict/bert_char_vocab\" \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 10000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir  \"/content/drive/MyDrive/NLP/output/wiki-eng/bert_base_cased_wiki_eng\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='/content/drive/MyDrive/NLP/data/CoNLL2003'\n",
    "MODEL_DIR='/content/drive/MyDrive/NLP/output/wiki-eng/bert_base_cased_wiki_eng'\n",
    "OUTPUT_DIR=='/content/drive/MyDrive/NLP/output/conll2003_ner'\n",
    "!python3 run_ner.py --data_dir ${DATA_DIR} \\\n",
    "                --model_type bert \\\n",
    "                --wandb_key $wandb_key \\\n",
    "                --wandb_project CharBERT \\\n",
    "                --wandb_run_name \"ner_conll2003\" \\\n",
    "                --model_name_or_path $MODEL_DIR \\\n",
    "                --output_dir ${OUTPUT_DIR} \\\n",
    "                --num_train_epochs 3 \\\n",
    "                --learning_rate 3e-5 \\\n",
    "                --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "                --per_gpu_train_batch_size 6 \\\n",
    "                --do_train \\\n",
    "                --do_predict \\\n",
    "                --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

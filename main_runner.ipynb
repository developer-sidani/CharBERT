{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive, userdata\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "github_token = userdata.get('github_token')\n",
    "github_username = userdata.get('github_username')\n",
    "try:\n",
    "  wandb_key = userdata.get('wandb_key')\n",
    "except userdata.SecretNotFoundError: \n",
    " wandb_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://$github_username:$github_token@github.com/developer-sidani/CharBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r CharBERT/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!mkdir data\n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
    "\n",
    "train_text = dataset['train'][:500]['text']\n",
    "eval_text = dataset['train'][500:650]['text']\n",
    "test_text = dataset['train'][650:800]['text']\n",
    "\n",
    "\n",
    "!mkdir data\n",
    "# Note that if you work on Colab you may need to create the folder \"data\" ---> change the directories based on your development tool of choice\n",
    "text = ''\n",
    "for el in train_text:\n",
    "  text += el\n",
    "with open(\"/content/data/train.txt\", 'w', encoding='utf-8') as f:\n",
    "  f.write(text)\n",
    "\n",
    "\n",
    "text = ''\n",
    "for el in eval_text:\n",
    "  text += el\n",
    "with open(\"/content/data/eval.txt\", 'w', encoding='utf-8') as f:\n",
    "  f.write(text)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "text = ''\n",
    "for el in test_text:\n",
    "  text += el\n",
    "with open(\"/content/data/test.txt\", 'w', encoding='utf-8') as f:\n",
    "  f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /content/CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-cased \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_data_file \"/content/data/train.txt\" \\\n",
    "    --eval_data_file  \"/content/data/eval.txt\" \\\n",
    "    --term_vocab \"/content/CharBERT/data/dict/term_vocab\" \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --char_vocab \"/content/CharBERT/data/dict/bert_char_vocab\" \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 10000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir  \"/content/output/bert_base_cased_wiki_eng\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

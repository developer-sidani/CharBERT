{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Imports and ENV setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive, userdata\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "github_token = userdata.get('github_token')\n",
    "github_username = userdata.get('github_username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://$github_username:$github_token@github.com/developer-sidani/CharBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r /content/CharBERT/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Define a function to check if the directory exists and print the dataset information if it does\n",
    "def check_and_create_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Dataset already exists at {path}.\")\n",
    "        return True\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return False\n",
    "\n",
    "# Define a function to download files if the directory does not exist\n",
    "def download_file(url, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        wget.download(url, output_path)\n",
    "\n",
    "# 1. Download CoNLL-2003 dataset\n",
    "def download_conll2003():\n",
    "    path = '/content/drive/MyDrive/data/CoNLL2003/'\n",
    "    if check_and_create_directory(path):\n",
    "        return\n",
    "\n",
    "    download_file(\"https://raw.githubusercontent.com/chnsh/BERT-NER-CoNLL/master/data/train.txt\", os.path.join(path, \"train.txt\"))\n",
    "    download_file(\"https://raw.githubusercontent.com/chnsh/BERT-NER-CoNLL/master/data/valid.txt\", os.path.join(path, \"val.txt\"))\n",
    "    download_file(\"https://raw.githubusercontent.com/chnsh/BERT-NER-CoNLL/master/data/valid.txt\", os.path.join(path, \"dev.txt\"))\n",
    "    download_file(\"https://raw.githubusercontent.com/chnsh/BERT-NER-CoNLL/master/data/test.txt\", os.path.join(path, \"test.txt\"))\n",
    "    print(\"CoNLL-2003 dataset downloaded.\")\n",
    "\n",
    "# 2. Download and process Italian CoNLL-2003 dataset\n",
    "def download_conll2003_ita():\n",
    "    path = '/content/drive/MyDrive/data/CoNLL2003_ita/'\n",
    "    if check_and_create_directory(path):\n",
    "        return\n",
    "\n",
    "    def conllu_to_txt(conllu_file, n_stop=100_000):\n",
    "        name_new_file = conllu_file.split(\".\")[0] + \".txt\"\n",
    "        with open(os.path.join(path, name_new_file), \"w\") as f_out:\n",
    "            with open(conllu_file, 'r', encoding='utf-8') as f_in:\n",
    "                for i, line in enumerate(f_in):\n",
    "                    if i >= n_stop:\n",
    "                        break\n",
    "                    if len(line.split(\"\\t\")) == 3:\n",
    "                        if line.split(\"\\t\")[0] == '0':\n",
    "                            f_out.write(\"\\n\")\n",
    "                        f_out.write(line.split(\"\\t\")[1] + \" \" + line.split(\"\\t\")[2])\n",
    "    \n",
    "    download_file(\"https://raw.githubusercontent.com/Babelscape/wikineural/master/data/wikineural/it/train.conllu\", \"train.conllu\")\n",
    "    download_file(\"https://raw.githubusercontent.com/Babelscape/wikineural/master/data/wikineural/it/val.conllu\", \"val.conllu\")\n",
    "    download_file(\"https://raw.githubusercontent.com/Babelscape/wikineural/master/data/wikineural/it/test.conllu\", \"test.conllu\")\n",
    "\n",
    "    conllu_to_txt(\"train.conllu\", n_stop=200_000)\n",
    "    conllu_to_txt(\"val.conllu\")\n",
    "    conllu_to_txt(\"test.conllu\")\n",
    "    os.remove(\"train.conllu\")\n",
    "    os.remove(\"val.conllu\")\n",
    "    os.remove(\"test.conllu\")\n",
    "    print(\"CoNLL-2003 Italian dataset processed.\")\n",
    "\n",
    "# 3. Download and process Wikipedia datasets\n",
    "def download_wikipedia():\n",
    "    path = '/content/drive/MyDrive/data/wiki/'\n",
    "    if check_and_create_directory(path):\n",
    "        return\n",
    "\n",
    "    def merge_and_shuffle(file1_path, file2_path, output_file_path):\n",
    "        with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
    "            content1 = file1.readlines()\n",
    "            content2 = file2.readlines()\n",
    "        merged_content = content1 + content2\n",
    "        random.shuffle(merged_content)\n",
    "\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.writelines(merged_content)\n",
    "\n",
    "    dataset = datasets.load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
    "    dataset = dataset.select(range(12000)).to_pandas()\n",
    "    train = dataset.iloc[:10000]\n",
    "    val = dataset.iloc[10000:11000]\n",
    "    test = dataset.iloc[11000:12000]\n",
    "\n",
    "    train.to_csv(os.path.join(path, 'wiki_eng_train.csv'), index=False)\n",
    "    val.to_csv(os.path.join(path, 'wiki_eng_val.csv'), index=False)\n",
    "    test.to_csv(os.path.join(path, 'wiki_eng_test.csv'), index=False)\n",
    "\n",
    "    little_train = dataset.iloc[:3500]\n",
    "    little_train.to_csv(os.path.join(path, 'wikil_eng_train.csv'), index=False)\n",
    "\n",
    "    dataset_it = datasets.load_dataset(\"wikipedia\", \"20220301.it\")['train']\n",
    "    dataset_it = dataset_it.select(range(2000)).to_pandas()\n",
    "    train_it = dataset_it.iloc[:1300]\n",
    "    val_it = dataset_it.iloc[1300:1600]\n",
    "    test_it = dataset_it.iloc[1600:2000]\n",
    "\n",
    "    train_it.to_csv(os.path.join(path, 'wiki_ita_train.csv'), index=False)\n",
    "    val_it.to_csv(os.path.join(path, 'wiki_ita_val.csv'), index=False)\n",
    "    test_it.to_csv(os.path.join(path, 'wiki_ita_test.csv'), index=False)\n",
    "\n",
    "    files = [\"wiki_eng_train.csv\", \"wiki_eng_val.csv\", \"wiki_eng_test.csv\", \"wikil_eng_train.csv\", \"wiki_ita_train.csv\", \"wiki_ita_val.csv\", \"wiki_ita_test.csv\"]\n",
    "    for file_ in files:\n",
    "        dataset = pd.read_csv(os.path.join(path, file_))\n",
    "        path_txt = os.path.join(path, f\"{file_.split('/')[-1].split('.')[0]}.txt\")\n",
    "        with open(path_txt, 'w') as f:\n",
    "            for row in dataset['text'][:len(dataset)//20]:\n",
    "                f.write(row)\n",
    "\n",
    "    merge_and_shuffle(os.path.join(path, 'wikil_eng_train.txt'), os.path.join(path, 'wiki_ita_train.txt'), os.path.join(path, 'wikil_eng_wiki_ita_train.txt'))\n",
    "    merge_and_shuffle(os.path.join(path, 'wiki_eng_val.txt'), os.path.join(path, 'wiki_ita_val.txt'), os.path.join(path, 'wikil_eng_wiki_ita_val.txt'))\n",
    "    merge_and_shuffle(os.path.join(path, 'wiki_eng_test.txt'), os.path.join(path, 'wiki_ita_test.txt'), os.path.join(path, 'wikil_eng_wiki_ita_test.txt'))\n",
    "\n",
    "    for file_ in [\"wiki_eng_train.txt\", \"wiki_eng_val.txt\", \"wiki_eng_test.txt\"]:\n",
    "        os.replace(file_, os.path.join(path, file_))\n",
    "\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(path, f))\n",
    "    print(\"Wikipedia datasets processed.\")\n",
    "\n",
    "# 4. Download AG News dataset\n",
    "def download_ag_news():\n",
    "    path = '/content/drive/MyDrive/data/news_domain/'\n",
    "    if check_and_create_directory(path):\n",
    "        return\n",
    "\n",
    "    dataset = datasets.load_dataset(\"ag_news\")['train'].shuffle(seed=42)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = int(0.1 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset = dataset.select(range(train_size))\n",
    "    val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "    test_dataset = dataset.select(range(train_size + val_size, len(dataset)))\n",
    "\n",
    "    def write_to_file(data_split, filename):\n",
    "        with open(os.path.join(path, filename), 'w', encoding=\"utf-8\") as f:\n",
    "            for example in data_split:\n",
    "                f.write(example['text'] + '\\n')\n",
    "\n",
    "    write_to_file(train_dataset, 'train.txt')\n",
    "    write_to_file(val_dataset, 'val.txt')\n",
    "    write_to_file(test_dataset, 'test.txt')\n",
    "    print(\"AG News dataset processed.\")\n",
    "\n",
    "# 5. Download WNUT 17 dataset\n",
    "def download_wnut_17():\n",
    "    path = '/content/drive/MyDrive/data/news_domain_ner/'\n",
    "    if check_and_create_directory(path):\n",
    "        return\n",
    "\n",
    "    dataset = datasets.load_dataset(\"wnut_17\")\n",
    "    val_test = dataset['validation']\n",
    "    val = val_test.shard(num_shards=2, index=0)\n",
    "    test = val_test.shard(num_shards=2, index=1)\n",
    "\n",
    "    dataset['validation'] = val\n",
    "    dataset['test'] = test\n",
    "\n",
    "    int_to_key = dataset['train'].features[\"ner_tags\"].feature.int2str\n",
    "\n",
    "    def write_to_file(split, filename):\n",
    "        with open(os.path.join(path, filename), 'w') as f:\n",
    "            all_tokens = dataset[split]['tokens']\n",
    "            all_tags = dataset[split]['ner_tags']\n",
    "            for tokens, tags in zip(all_tokens, all_tags):\n",
    "                for token, tag in zip(tokens, tags):\n",
    "                    f.write(f'{token} {int_to_key(tag)}\\n')\n",
    "                f.write('\\n')\n",
    "\n",
    "    write_to_file('train', 'train.txt')\n",
    "    write_to_file('validation', 'val.txt')\n",
    "    write_to_file('test', 'test.txt')\n",
    "\n",
    "    with open(os.path.join(path, 'labels.txt'), 'w') as f:\n",
    "        labels = dataset['train'].features['ner_tags'].feature.names\n",
    "        for label in labels[:-1]:\n",
    "            f.write(label + '\\n')\n",
    "        f.write(labels[-1])\n",
    "    print(\"WNUT 17 dataset processed.\")\n",
    "\n",
    "# Main function to download all datasets\n",
    "def main():\n",
    "    download_conll2003()\n",
    "    download_conll2003_ita()\n",
    "    download_wikipedia()\n",
    "    download_ag_news()\n",
    "    download_wnut_17()\n",
    "\n",
    "if __name__ == \"main\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning on wikidataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /content/CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-cased \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_data_file /content/drive/MyDrive/NLP/data/wiki/wiki_eng_train.txt \\\n",
    "    --eval_data_file  /content/drive/MyDrive/NLP/data/wiki/wiki_eng_eval.txt \\\n",
    "    --term_vocab /content/CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 10000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir  /content/drive/MyDrive/NLP/output/wiki-eng/bert_base_cased_wiki_eng\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ner CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /content/CharBERT/run_ner.py --data_dir /content/drive/MyDrive/NLP/data/CoNLL2003 \\\n",
    "                --model_type bert \\\n",
    "                --model_name_or_path /content/drive/MyDrive/NLP/output/wiki-eng/bert_base_cased_wiki_eng \\\n",
    "                --output_dir /content/drive/MyDrive/NLP/output/conll2003_ner \\\n",
    "                --num_train_epochs 1 \\\n",
    "                --learning_rate 3e-5 \\\n",
    "                --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "                --per_gpu_train_batch_size 6 \\\n",
    "                --do_train \\\n",
    "                --do_predict \\\n",
    "                --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning on wikidataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /content/CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type roberta \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_data_file /content/drive/MyDrive/NLP/data/wiki/wiki_eng_train.txt \\\n",
    "    --eval_data_file  /content/drive/MyDrive/NLP/data/wiki/wiki_eng_eval.txt \\\n",
    "    --term_vocab /content/CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --char_vocab /content/CharBERT/data/dict/roberta_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 10000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir /content/drive/MyDrive/NLP/output/wiki-eng/robert_base_wiki_eng\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ner CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /content/CharBERT/run_ner.py --data_dir /content/drive/MyDrive/NLP/data/CoNLL2003 \\\n",
    "                --model_type roberta \\\n",
    "                --model_name_or_path /content/drive/MyDrive/NLP/output/wiki-eng/robert_base_wiki_eng \\\n",
    "                --output_dir /content/drive/MyDrive/NLP/output/conll2003_ner \\\n",
    "                --num_train_epochs 1 \\\n",
    "                --learning_rate 3e-5 \\\n",
    "                --char_vocab /content/CharBERT/data/dict/roberta_char_vocab \\\n",
    "                --per_gpu_train_batch_size 6 \\\n",
    "                --do_train \\\n",
    "                --do_predict \\\n",
    "                --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adaptation:\n",
    "\n",
    "Running domain adaptation on AG News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune on News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-cased \\\n",
    "    --output_dir /content/drive/MyDrive/NLP/output/news/mlm_bert_base/ \\\n",
    "    --train_data_file /content/drive/MyDrive/NLP/data/news_domain/train.txt \\\n",
    "    --eval_data_file  /content/drive/MyDrive/NLP/data/news_domain/val.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --term_vocab /content/CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 2000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER on CoNLL-2003 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python /content/CharBERT/run_ner.py --data_dir /content/drive/MyDrive/NLP/data/CoNLL2003/ \\\n",
    "                            --model_type bert \\\n",
    "                            --model_name_or_path /content/drive/MyDrive/NLP/output/news/mlm_bert_base/ \\\n",
    "                            --output_dir /content/drive/MyDrive/NLP/output/news/NER_cased/conll2003 \\\n",
    "                            --num_train_epochs 1 \\\n",
    "                            --learning_rate 3e-5 \\\n",
    "                            --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "                            --per_gpu_train_batch_size 6 \\\n",
    "                            --do_train \\\n",
    "                            --do_predict \\\n",
    "                            --overwrite_output_dir \\\n",
    "                            --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER on WNUT 17 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_ner.py --data_dir /content/drive/MyDrive/NLP/data/legal_domain_ner/ \\\n",
    "                            --model_type bert \\\n",
    "                            --labels /content/drive/MyDrive/NLP/data/legal_domain_ner/labels.txt \\\n",
    "                            --model_name_or_path /content/drive/MyDrive/NLP/output/news/mlm_bert_base/ \\\n",
    "                            --output_dir /content/drive/MyDrive/NLP/output/news/NER_cased/legal_domain_ner \\\n",
    "                            --num_train_epochs 1 \\\n",
    "                            --learning_rate 3e-5 \\\n",
    "                            --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "                            --per_gpu_train_batch_size 6 \\\n",
    "                            --do_train \\\n",
    "                            --do_predict \\\n",
    "                            --overwrite_output_dir \\\n",
    "                            --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-cased-ag-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone the PLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/lucasresck/bert-base-cased-ag-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune on News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path /content/bert-base-cased-ag-news \\\n",
    "    --output_dir /content/drive/MyDrive/NLP/output/news/bert-base-cased-ag-news/ \\\n",
    "    --train_data_file /content/drive/MyDrive/NLP/data/news_domain/train.txt \\\n",
    "    --eval_data_file  /content/drive/MyDrive/NLP/data/news_domain/val.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --term_vocab /content/CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 2000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER on CoNLL-2003 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_ner.py --data_dir /content/drive/MyDrive/NLP/data/CoNLL2003/ \\\n",
    "                            --model_type bert \\\n",
    "                            --model_name_or_path /content/drive/MyDrive/NLP/output/news/bert-base-cased-ag-news/ \\\n",
    "                            --output_dir /content/drive/MyDrive/NLP/output/news/NER_cased/conll2003_bert-base-cased-ag-news \\\n",
    "                            --num_train_epochs 1 \\\n",
    "                            --learning_rate 3e-5 \\\n",
    "                            --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "                            --per_gpu_train_batch_size 6 \\\n",
    "                            --do_train \\\n",
    "                            --do_predict \\\n",
    "                            --overwrite_output_dir \\\n",
    "                            --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER on WNUT 17 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_ner.py --data_dir /content/drive/MyDrive/NLP/data/legal_domain_ner/ \\\n",
    "                            --model_type bert \\\n",
    "                            --labels /content/drive/MyDrive/NLP/data/legal_domain_ner/labels.txt \\\n",
    "                            --model_name_or_path /content/drive/MyDrive/NLP/output/news/bert-base-cased-ag-news/ \\\n",
    "                            --output_dir /content/drive/MyDrive/NLP/output/news/NER_cased/legal_domain_ner_bert-base-cased-ag-news \\\n",
    "                            --num_train_epochs 1 \\\n",
    "                            --learning_rate 3e-5 \\\n",
    "                            --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "                            --per_gpu_train_batch_size 6 \\\n",
    "                            --do_train \\\n",
    "                            --do_predict \\\n",
    "                            --overwrite_output_dir \\\n",
    "                            --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune on News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type roberta \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --output_dir /content/drive/MyDrive/NLP/output/news/mlm_roberta_base/ \\\n",
    "    --train_data_file /content/drive/MyDrive/NLP/data/news_domain/train.txt \\\n",
    "    --eval_data_file  /content/drive/MyDrive/NLP/data/news_domain/val.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --term_vocab /content/CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --char_vocab /content/CharBERT/data/dict/roberta_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 2000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER on CoNLL-2003 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_ner.py --data_dir /content/drive/MyDrive/NLP/data/CoNLL2003/ \\\n",
    "                            --model_type roberta \\\n",
    "                            --model_name_or_path /content/drive/MyDrive/NLP/output/news/mlm_roberta_base/ \\\n",
    "                            --output_dir /content/drive/MyDrive/NLP/output/news/NER_cased/conll2003 \\\n",
    "                            --num_train_epochs 1 \\\n",
    "                            --learning_rate 3e-5 \\\n",
    "                            --char_vocab /content/CharBERT/data/dict/roberta_char_vocab \\\n",
    "                            --per_gpu_train_batch_size 6 \\\n",
    "                            --do_train \\\n",
    "                            --do_predict \\\n",
    "                            --overwrite_output_dir \\\n",
    "                            --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER on WNUT 17 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_ner.py --data_dir /content/drive/MyDrive/NLP/data/legal_domain_ner/ \\\n",
    "                            --model_type roberta \\\n",
    "                            --labels /content/drive/MyDrive/NLP/data/legal_domain_ner/labels.txt \\\n",
    "                            --model_name_or_path /content/drive/MyDrive/NLP/output/news/mlm_roberta_base/ \\\n",
    "                            --output_dir /content/drive/MyDrive/NLP/output/news/NER_cased/legal_domain_ner \\\n",
    "                            --num_train_epochs 1 \\\n",
    "                            --learning_rate 3e-5 \\\n",
    "                            --char_vocab /content/CharBERT/data/dict/roberta_char_vocab \\\n",
    "                            --per_gpu_train_batch_size 6 \\\n",
    "                            --do_train \\\n",
    "                            --do_predict \\\n",
    "                            --overwrite_output_dir \\\n",
    "                            --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roberta-base-ag-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone the PLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/textattack/roberta-base-ag-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune on News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type roberta \\\n",
    "    --model_name_or_path /content/roberta-base-ag-news \\\n",
    "    --output_dir /content/drive/MyDrive/NLP/output/news/roberta-base-ag-news/ \\\n",
    "    --train_data_file /content/drive/MyDrive/NLP/data/news_domain/train.txt \\\n",
    "    --eval_data_file  /content/drive/MyDrive/NLP/data/news_domain/val.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --term_vocab /content/CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --char_vocab /content/CharBERT/data/dict/roberta_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 2000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER on CoNLL-2003 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_ner.py --data_dir /content/drive/MyDrive/NLP/data/CoNLL2003/ \\\n",
    "                            --model_type roberta \\\n",
    "                            --model_name_or_path /content/drive/MyDrive/NLP/output/news/roberta-base-ag-news/ \\\n",
    "                            --output_dir /content/drive/MyDrive/NLP/output/news/NER_cased/conll2003_roberta-base-ag-news \\\n",
    "                            --num_train_epochs 1 \\\n",
    "                            --learning_rate 3e-5 \\\n",
    "                            --char_vocab /content/CharBERT/data/dict/roberta_char_vocab \\\n",
    "                            --per_gpu_train_batch_size 6 \\\n",
    "                            --do_train \\\n",
    "                            --do_predict \\\n",
    "                            --overwrite_output_dir \\\n",
    "                            --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER on WNUT 17 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_ner.py --data_dir /content/drive/MyDrive/NLP/data/legal_domain_ner/ \\\n",
    "                            --model_type roberta \\\n",
    "                            --labels /content/drive/MyDrive/NLP/data/legal_domain_ner/labels.txt \\\n",
    "                            --model_name_or_path /content/drive/MyDrive/NLP/output/news/roberta-base-ag-news/ \\\n",
    "                            --output_dir /content/drive/MyDrive/NLP/output/news/NER_cased/legal_domain_ner_roberta-base-ag-news \\\n",
    "                            --num_train_epochs 1 \\\n",
    "                            --learning_rate 3e-5 \\\n",
    "                            --char_vocab /content/CharBERTdata/dict/roberta_char_vocab \\\n",
    "                            --per_gpu_train_batch_size 6 \\\n",
    "                            --do_train \\\n",
    "                            --do_predict \\\n",
    "                            --overwrite_output_dir \\\n",
    "                            --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune on multilingual Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /content/CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_data_file /content/drive/MyDrive/NLP/data/wiki/wikil_eng_wiki_ita_train.txt \\\n",
    "    --eval_data_file  /content/drive/MyDrive/NLP/data/wiki/wikil_eng_wiki_ita_val.txt \\\n",
    "    --term_vocab /content/CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 10000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir  /content/drive/MyDrive/NLP/output/multilingual/MLM_cased/wikil_eng_wiki_ita     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ner Multilingual CoNLL2003 ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /content/CharBERT/run_ner.py \\\n",
    "                    --data_dir /content/drive/MyDrive/NLP/data/CoNLL2003 \\\n",
    "                    --model_type bert \\\n",
    "                    --model_name_or_path /content/drive/MyDrive/NLP/output/multilingual/MLM_cased/wikil_eng_wiki_ita \\\n",
    "                    --output_dir  /content/drive/MyDrive/NLP/output/multilingual/NER_cased/conll2003 \\\n",
    "                    --num_train_epochs 1 \\\n",
    "                    --learning_rate 3e-5 \\\n",
    "                    --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "                    --per_gpu_train_batch_size 6 \\\n",
    "                    --do_train \\\n",
    "                    --do_predict \\\n",
    "                    --overwrite_output_dir \\\n",
    "                    --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ner Multilingual CoNLL2003 Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_ner.py \\\n",
    "                    --data_dir /content/drive/MyDrive/NLP/data/CoNLL2003_ita \\\n",
    "                    --model_type bert \\\n",
    "                    --model_name_or_path /content/drive/MyDrive/NLP/output/multilingual/MLM_cased/wikil_eng_wiki_ita \\\n",
    "                    --output_dir  /content/drive/MyDrive/NLP/output/multilingual/NER_cased/conll2003_ita \\\n",
    "                    --num_train_epochs 1 \\\n",
    "                    --learning_rate 3e-5 \\\n",
    "                    --char_vocab /content/CharBERT/data/dict/bert_char_vocab \\\n",
    "                    --per_gpu_train_batch_size 6 \\\n",
    "                    --do_train \\\n",
    "                    --do_predict \\\n",
    "                    --overwrite_output_dir \\\n",
    "                    --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune on multilingual Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /content/CharBERT/run_lm_finetuning.py \\\n",
    "    --model_type roberta \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_data_file /content/drive/MyDrive/NLP/data/wiki/wikil_eng_wiki_ita_train.txt \\\n",
    "    --eval_data_file  /content/drive/MyDrive/NLP/data/wiki/wikil_eng_wiki_ita_val.txt \\\n",
    "    --term_vocab /content/CharBERT/data/dict/term_vocab \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --char_vocab /content/CharBERT/data/dict/roberta_char_vocab \\\n",
    "    --mlm_probability 0.10 \\\n",
    "    --input_nraws 1000 \\\n",
    "    --per_gpu_train_batch_size 4 \\\n",
    "    --per_gpu_eval_batch_size 4 \\\n",
    "    --save_steps 10000 \\\n",
    "    --block_size 384 \\\n",
    "    --mlm \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir  /content/drive/MyDrive/NLP/output/multilingual/MLM_cased/wikil_eng_wiki_ita     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ner Multilingual CoNLL2003 ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /content/CharBERT/run_ner.py \\\n",
    "                    --data_dir /content/drive/MyDrive/NLP/data/CoNLL2003 \\\n",
    "                    --model_type roberta \\\n",
    "                    --model_name_or_path /content/drive/MyDrive/NLP/output/multilingual/MLM_cased/wikil_eng_wiki_ita \\\n",
    "                    --output_dir  /content/drive/MyDrive/NLP/output/multilingual/NER_cased/conll2003 \\\n",
    "                    --num_train_epochs 1 \\\n",
    "                    --learning_rate 3e-5 \\\n",
    "                    --char_vocab /content/CharBERT/data/dict/roberta_char_vocab \\\n",
    "                    --per_gpu_train_batch_size 6 \\\n",
    "                    --do_train \\\n",
    "                    --do_predict \\\n",
    "                    --overwrite_output_dir \\\n",
    "                    --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ner Multilingual CoNLL2003 Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CharBERT/run_ner.py \\\n",
    "                    --data_dir /content/drive/MyDrive/NLP/data/CoNLL2003_ita \\\n",
    "                    --model_type roberta \\\n",
    "                    --model_name_or_path /content/drive/MyDrive/NLP/output/multilingual//MLM_cased/wikil_eng_wiki_ita \\\n",
    "                    --output_dir  /content/drive/MyDrive/NLP/output/multilingual/NER_cased/conll2003_ita \\\n",
    "                    --num_train_epochs 1 \\\n",
    "                    --learning_rate 3e-5 \\\n",
    "                    --char_vocab /content/CharBERT/data/dict/roberta_char_vocab \\\n",
    "                    --per_gpu_train_batch_size 6 \\\n",
    "                    --do_train \\\n",
    "                    --do_predict \\\n",
    "                    --overwrite_output_dir \\\n",
    "                    --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Runtime from Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
